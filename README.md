# ğŸ“˜ ChatPDF â€“ Mini PDF Q&A App

<img width="1727" alt="Screenshot" src="https://github.com/trangiabach/chat-pdf/assets/62537937/e0518c85-295f-4b39-8254-0b9b05a93f49">

[Walkthrough Video](https://drive.google.com/file/d/1fSoZTbu8a_s8J6-6utZfVOLvijlDH97j/view?usp=sharing)

---

## ğŸ¯ Objective
This project is a **full-stack Next.js application** that allows users to:
- Upload PDF files and process their text  
- Generate **OpenAI embeddings** and store them in a **Supabase pgvector** database  
- Ask natural-language questions about the PDF  
- Retrieve contextual answers using a **retrieval-augmented generation (RAG)** pipeline  

Built as part of the **Force Equals Full Stack AI Engineer Intern Assignment**.

---

## ğŸš€ Features
- ğŸ“‚ Upload one or multiple PDFs  
- ğŸ§  Extract and embed text using OpenAI Embeddings API  
- ğŸ“¦ Store embeddings in **Supabase pgvector**  
- ğŸ¤– RAG-based Q&A with OpenAI + LangChain  
- ğŸ”’ Protected API routes with simple login  
- âš¡ Streaming answers (token-by-token)  
- ğŸ¨ Modern responsive UI (Next.js + Tailwind + shadcn/ui)  
- ğŸŒ™ Theme switching (light/dark)  
- ğŸ“± Mobile responsive  

---

## âš™ï¸ Local Development

### 1. Clone the repository
```bash
git clone https://github.com/Harshiitmadras/Chat_pdf.git
cd Chat_pdf
---
##2. Install dependencies

npm install

###3. Setup environment variables

Create a file named .env.local in the root of your project:

OPENAI_API_KEY=your-openai-key
SUPABASE_URL=your-supabase-url
SUPABASE_KEY=your-supabase-service-role-key
ADMIN_KEY=your-secret-password

###4. Run the development server

npm run dev

Now visit http://localhost:3000 ğŸš€


---

##âš™ï¸ Approach & Architecture

#1. PDF Processing & Embedding (app/api/pdfs/embed)

Uploaded PDFs are processed server-side with pdfjs.

Text is split into smaller chunks (â‰¤ 4000 tokens).

Each chunk is embedded with OpenAI and stored in Supabase pgvector along with metadata.

The raw PDF is stored in a Supabase bucket for later use.


#2. Retrieval-Augmented Generation (RAG) (app/api/chat)

User sends a query, along with the chosen PDF and model.

The query is embedded, and the top 3 most similar text chunks are retrieved.

These chunks are merged into a context window along with chat history.

The prompt + context are sent to the LLM.

The LLM streams back an answer token-by-token to the frontend.



---

#ğŸ“‚ Project Structure

Chat_pdf/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/          # API routes (upload, embed, chat)
â”‚   â”œâ”€â”€ components/   # Reusable UI components
â”‚   â”œâ”€â”€ page.tsx      # Main UI page
â”œâ”€â”€ lib/              # OpenAI + Supabase helpers
â”œâ”€â”€ providers/        # Context providers
â”œâ”€â”€ db/               # Supabase schema & connection
â”œâ”€â”€ types/            # TypeScript types
â”œâ”€â”€ public/           # Static assets
â”œâ”€â”€ middleware.ts     # Protects API routes
â”œâ”€â”€ .env.example      # Env template
â””â”€â”€ README.md


---

#ğŸ”’ Authentication

API routes (/api/upload, /api/chat) are protected with a simple ADMIN_KEY.

On login, an HttpOnly cookie is set â†’ required for further API access.

This is demo authentication only.

For production, consider NextAuth / Auth0 / Clerk.



---

#ğŸ› ï¸ Tech Stack

Frontend: Next.js (React, TypeScript), TailwindCSS, shadcn/ui

Backend: Next.js API Routes (Serverless)

Database: Supabase (pgvector + storage)

AI / LLM: OpenAI API, LangChain

Streaming: Vercel AI SDK



---
